{
    "text_segment": "You have an ML model designed for an industrial company that provides the correct price to buy goods based on a series of elements, such as the quantity requested, the level of quality and other specific variables for different types of products. You have built a linear regression model that works well but whose performance you want to optimize. Which of these techniques could you use?\nClipping\nLog scaling\nZ-score\nScaling to a range\nTu respuesta es correcta\nAll of them\nExplicaci\u00f3n general\nFeature clipping eliminates outliers that are too high or too low. Scaling means transforming feature values into a standard range, from 0 and 1 or sometimes -1 to +1. It\u2018s okay when you have an even distribution between minimum and maximum. When you don\u2018t have a fairly uniform distribution, you can instead use Log Scaling which can compress the data range: x1 = log (x) Z-Score is similar to scaling, but uses the deviation from the mean divided by the standard deviation, which is the classic index of variability. So, it gives how many standard deviations each value is away from the mean. All these methods maintain the differences between values, but limit the range. So the computation is lighter.\nPregunta 32\nIncorrecto\nYou are developing ML models with AI Platform for image segmentation on CT scans. You frequently update your model architectures based on the newest available research papers, and have to rerun training on the same dataset to benchmark their performance. You want to minimize computation costs and manual intervention while having version control for your code. What should you do?\nTu respuesta es incorrecta\nUse Cloud Functions to identify changes to your code in Cloud Storage and trigger a retraining job.\nUse the gcloud command-line tool to submit training jobs on AI Platform when you update your code.\nRespuesta correcta\nUse Cloud Build linked with Cloud Source Repositories to trigger retraining when new code is pushed to the repository.\nCreate an automated workflow in Cloud Composer that runs daily and looks for changes in code in Cloud Storage using a sensor.\nExplicaci\u00f3n general\nCI/CD for Kubeflow pipelines. At the heart of this architecture is Cloud Build, infrastructure. Cloud Build can import source from Cloud Source Repositories, GitHub, or Bitbucket, and then execute a build to your specifications, and produce artifacts such as Docker containers or Python tar files.\nPregunta 33\nIncorrecto\nWhile running a model training pipeline on Vertex Al, you discover that the evaluation step is failing because of an out-of-memory error. You are currently using TensorFlow Model Analysis (TFMA) with a standard Evaluator TensorFlow Extended (TFX) pipeline component for the evaluation step. You want to stabilize the pipeline without downgrading the evaluation quality while minimizing infrastructure overhead. What should you do?\nRespuesta correcta\nInclude the flag -runner=DataflowRunner in beam_pipeline_args to run the evaluation step on Dataflow.\nMove the evaluation step out of your pipeline and run it on custom Compute Engine VMs with sufficient memory.\nMigrate your pipeline to Kubeflow hosted on Google Kubernetes Engine, and specify the appropriate node parameters for the evaluation step.\nTu respuesta es incorrecta\nAdd tfma.MetricsSpec () to limit the number of metrics in the evaluation step.\nExplicaci\u00f3n general\n\"Evaluator leverages the TensorFlow Model Analysis library to perform the analysis, which in turn use Apache Beam for scalable processing.\" Since Dataflow is Google Cloud's serverless Apache Beam offering, this option can easily be implemented to address the issue while leaving the evaluation logic as such identical https://www.tensorflow.org/tfx/guide/evaluator#evaluator_and_tensorflow_model_analysis\nPregunta 34\nCorrecto\nTo reduce the amount of effort involved in constructing an inference pipeline, you should run a batch prediction on a BigQuery table with a custom TensorFlow DNN regressor model, containing 100 million records. Subsequently, store the predicted results in another BigQuery table. What is the best way to approach this task?\nLoad the TensorFlow SavedModel in a Dataflow pipeline. Utilize the BigQuery I/O connector with a custom function to conduct the inference within the pipeline, and write the results to BigQuery.\nUse the TensorFlow BigQuery reader to fetch the data, and use the BigQuery API to save the results to BigQuery.\nBuild a Dataflow pipeline to convert the data in BigQuery to TFRecords. Execute a batch inference on Vertex AI Prediction, and write the results to BigQuery.\nTu respuesta es correcta\nImport the TensorFlow model with BigQuery ML, and invoke the ml.predict function.\nExplicaci\u00f3n general\nThis is the correct answer because BigQuery ML makes it easy to deploy ML models in BigQuery, allowing you to use SQL queries to run batch predictions on data stored in BigQuery. BigQuery ML comes with a built-in TensorFlow model, so you can use the ml.predict function to execute a batch prediction on the data stored in BigQuery with a custom TensorFlow DNN regressor model. This enables you to quickly and easily create an inference pipeline without needing to manually write code.\nPregunta 35\nIncorrecto\nYour company runs a big retail website. You develop many ML models for all the business activities.You migrated to Google Cloud. Your models are developed with PyTorch, TensorFlow, and BigQueryML. You also use BigTable and CloudSQL, and Cloud Storage, of course. You need to use input tabular data in CSV format. You are working with Vertex AI. How do you manage them in the best way (pick 2)?\nTu selecci\u00f3n es incorrecta\nVertex AI manage any CSV automatically, no operations needed\nTu selecci\u00f3n es correcta\nYou have to setup an header and column names may have only alphanumeric character and underscore\nVertex AI cannot handle CSV files\nSelecci\u00f3n correcta\nDelimiter must be a comma\nYou can import only a file max 10GB\nExplicaci\u00f3n general\nVertex AI manages CSV files automatically. But you need to have headers only with alphanumeric characters and underscores with commas as delimiters.So, A and C are wrong.You can import multiple files, each one max 10GB.So, E is wrong.For any further detail:https://cloud.google.com/vertex-ai/docs/datasets/prepare-tabular#csvhttps://cloud.google.com/vertex-ai/docs/datasets/datasets\nPregunta 36\nIncorrecto\nYou are developing a Kubeflow pipeline on Google Kubernetes Engine. The first step in the pipeline is to issue a query against BigQuery. You plan to use the results of that query as the input to the next step in your pipeline. You want to achieve this in the easiest way possible. What should you do?\nTu respuesta es incorrecta\nUse the BigQuery console to execute your query, and then save the query results into a new BigQuery table.\nWrite a Python script that uses the BigQuery API to execute queries against BigQuery. Execute this script as the first step in your Kubeflow pipeline.\nUse the Kubeflow Pipelines domain-specific language to create a custom component that uses the Python BigQuery client library to execute queries.\nRespuesta correcta\nLocate the Kubeflow Pipelines repository on GitHub. Find the BigQuery Query Component, copy that component's URL, and use it to load the component into your pipeline. Use the component to execute queries against BigQuery.\nExplicaci\u00f3n general\nLocate the Kubeflow Pipelines repository on GitHub. Find the BigQuery Query Component, copy that component's URL, and use it to load the component into your pipeline. Use the component to execute queries against BigQuery.\nPregunta 37\nIncorrecto\nYou work for an online publisher that delivers news articles to over 50 million readers. You have built an AI model that recommends content for the company's weekly newsletter. A recommendation is considered successful if the article is opened within two days of the newsletter's published date and the user remains on the page for at least one minute.All the information needed to compute the success metric is available in BigQuery and is updated hourly. The model is trained on eight weeks of data, on average its performance degrades below the acceptable baseline after five weeks, and training time is 12 hours. You want to ensure that the model's performance is above the acceptable baseline while minimizing cost. How should you monitor the model to determine when retraining is necessary?\nTu respuesta es incorrecta\nUse Vertex AI Model Monitoring to detect skew of the input features with a sample rate of 100% and a monitoring frequency of two days.\nSchedule a cron job in Cloud Tasks to retrain the model every week before the newsletter is created.\nRespuesta correcta\nSchedule a weekly query in BigQuery to compute the success metric.\nSchedule a daily Dataflow job in Cloud Composer to compute the success metric.\nExplicaci\u00f3n general\nSince all the information needed to compute the success metric is available in BigQuery and is updated hourly, scheduling a weekly query in BigQuery to compute the success metric is the simplest and most cost-effective way to monitor the model's performance. By comparing the computed success metric against the acceptable baseline, you can determine when the model's performance has degraded below the threshold, and retrain the model accordingly. This approach avoids the cost of additional monitoring infrastructure and leverages existing data processing capabilities.\nPregunta 38\nCorrecto\nTo optimize the efficiency of the production demand forecasting pipeline, Z-score normalization is used on data stored in BigQuery during preprocessing. Every week, new training data is added. To minimize computation time and manual intervention, what steps should be taken?\nNormalize the data with Apache Spark using the Dataproc connector for BigQuery.\nUtilize the normalizer_fn argument in TensorFlow's Feature Column API.\nTu respuesta es correcta\nConvert the normalization algorithm into SQL for use with BigQuery.\nNormalize the data using Google Kubernetes Engine.\nExplicaci\u00f3n general\nThis is the correct answer because using SQL to normalize the data in BigQuery can help minimize computation time and manual intervention by eliminating the need to export and transform the data before preprocessing. By performing the Z-score normalization in BigQuery instead, you can save time and resources while ensuring that the data is normalized in a consistent manner.\nPregunta 39\nOmitido\nThe purpose of your current project is the recognition of genuine or forged signatures on checks and documents against regular signatures already stored by the Bank. There is obviously a very low incidence of fake signatures. The system must recognize which customer the signature belongs to and whether the signature is identified as genuine or skilled forged. Which of the following technical specifications can't you use with CNN?\nKernel Selection\nRespuesta correcta\nFeature Cross\nStride\nMax pooling layer\nExplicaci\u00f3n general\nA cross of functions is a dome that creates new functions by multiplying (crossing) two or more functions. It has proved to be an important technique and is also used to introduce non-linearity to the model. We don't need it in our case. Filters or kernels are a computation on a sub-matrix of pixels. Stride is obtained by sliding the kernel by 1 pixel. A Max pooling layer is created taking the max value of a small region. It is used for simplification. Dropout is also for simplification or regularization. It randomly zeroes some of the matrix values in order to find out what can be discarded with minor loss (and no overfitting)\nPregunta 40\nIncorrecto\nAs an ML engineer employed by a major grocery retailer, you have been tasked to construct an inventory prediction model. This model should be able to use several features such as region, location, past demand, and seasonal trends to determine the predicted inventory level. Moreover, the algorithm should be capable of learning from daily new inventory data. Which algorithms would be best suited to build this model?\nTu respuesta es incorrecta\nReinforcement Learning\nConvolutional Neural Networks (CNN)\nClassification\nRespuesta correcta\nRecurrent Neural Networks (RNN)\nExplicaci\u00f3n general\nThis is the correct answer because Recurrent Neural Networks (RNN) are well-suited for modeling sequential data, such as inventory data that changes on a daily basis. RNNs can learn from new data points and incorporate them into the prediction model, which is exactly what is needed in this scenario. Furthermore, RNNs have the ability to process data from multiple sources and make predictions based on that data. This makes RNNs a great choice for this prediction model since it will be using multiple features such as region, location, historical demand, and seasonal popularity.",
    "flashcard_structure": [
        {
            "Answer": "Opci\u00f3n correcta completa ",
            "Explanation": "Explicaci\u00f3n completa de la respuesta correcta, sin omisiones por favor, manteniendo el texto proporcionado. Agrega s\u00f3lo contexto adicional si es necesario para claridad.",
            "Option1": "Opci\u00f3n 1 completa",
            "Option2": "Opci\u00f3n 2 completa",
            "Option3": "Opci\u00f3n 3 completa",
            "Option4": "Opci\u00f3n 4 completa",
            "Question": "Texto completo de la pregunta exactamente como se encuentra en el texto de entrada, sin omisiones por favor",
            "Tags": [
                "Etiquetas espec\u00edficas y relevantes separadas por comas, como 'NLP', 'Data Splitting', etc."
            ]
        }
    ],
    "instructions": "Answer debe copiarse exactamente tal como est\u00e1 en el texto original, sin modificar el texto por favor.\nIncluye las cuatro opciones completas en los campos Option1 a Option4, asegurando que una de ellas es la correcta.\nUsa el campo Answer para la opci\u00f3n correcta completa y tal como aparece en el texto.\nLa explicaci\u00f3n (Explanation) debe incluir el texto dado en el archivo original, sin omisiones por favor. seguido de un contexto adicional si es necesario.\nLas etiquetas (Tags) deben estar en ingl\u00e9s y reflejar los temas de la pregunta.\n",
    "tags_suggested": "# Fundamentals of Machine Learning\n\"ML Algorithms\", \"Supervised Learning\", \"Unsupervised Learning\", \"Reinforcement Learning\", \"Model Evaluation\",\n# Model Development and Training\n\"Data Preprocessing\", \"Feature Engineering\", \"Model Training\", \"Hyperparameter Tuning\", \"Overfitting and Underfitting\",\n# Infrastructure and Scalability\n\"Google Cloud ML Tools\", \"AI Platform\", \"BigQuery ML\", \"TensorFlow Extended (TFX)\", \"Data Pipelines\",\n# Deployment and Operations\n\"Model Deployment\", \"AI Platform Serving\", \"Model Monitoring\", \"Model Maintenance\", \"Optimization\",\n# Ethics and Security\n\"Bias and Fairness\", \"Explainability\", \"Data Privacy\", \"Model Security\",\n# Google Cloud Specifics\n\"IAM\", \"VPC\", \"Compute Engine\", \"Cloud Storage\", \"BigQuery\", \"Dataflow\", \"Pub/Sub\", \"Cloud Functions\",\n# Theory and Mathematics\n\"Probability\", \"Distributions\", \"Inferential Statistics\", \"Linear Algebra\", \"Calculus\", \"Gradient Descent\",\n# Miscellaneous\n\"Jupyter Notebooks\", \"Colab\", \"Git\", \"Docker\", \"Python\", \"SQL\"",
    "image_path_format": "\" \"",
    "expected_output_example": [
        {
            "Answer": "D",
            "Explanation": "To detect anomalies in real-time sensor data, you can use Dataflow to process the data, AI Platform to run ML models, and BigQuery to store the results for analytics and visualization.",
            "Image": "",
            "Option1": "1 = DataProc, 2 = AutoML, 3 = Cloud Bigtable",
            "Option2": "1 = BigQuery, 2 = AutoML, 3 = Cloud Functions",
            "Option3": "1 = BigQuery, 2 = AI Platform, 3 = Cloud Storage",
            "Option4": "1 = Dataflow, 2 = AI Platform, 3 = BigQuery",
            "Question": "How should you configure the pipeline to detect anomalies in real-time sensor data using Pub/Sub and store the results for analytics and visualization?",
            "Tags": [
                "Google Cloud ML Tools",
                "Data Pipelines",
                "Model Deployment",
                "BigQuery",
                "Dataflow"
            ]
        },
        {
            "Answer": "A",
            "Explanation": "The best approach is to define the optimal route as the shortest route that passes by all shuttle stations with confirmed attendance at the given time under capacity constraints, and then dispatch an appropriately sized shuttle indicating the required stops on the map.",
            "Image": "",
            "Option1": "1. Define the optimal route as the shortest route that passes by all shuttle stations with confirmed attendance at the given time under capacity constraints. 2. Dispatch an appropriately sized shuttle and indicate the required stops on the map.",
            "Option2": "1. Build a reinforcement learning model with tree-based classification models that predict the presence of passengers at shuttle stops as agents and a reward function around a distance-based metric. 2. Dispatch an appropriately sized shuttle and provide the map with the required stops based on the simulated outcome.",
            "Option3": "1. Build a tree-based regression model that predicts how many passengers will be picked up at each shuttle station. 2. Dispatch an appropriately sized shuttle and provide the map with the required stops based on the prediction.",
            "Option4": "1. Build a tree-based classification model that predicts whether the shuttle should pick up passengers at each shuttle station. 2. Dispatch an available shuttle and provide the map with the required stops based on the prediction.",
            "Question": "What approach should you take to make your internal shuttle service route more efficient using Google Kubernetes Engine?",
            "Tags": [
                "Google Cloud ML Tools",
                "Model Training",
                "Model Deployment",
                "Optimization",
                "Google Kubernetes Engine"
            ]
        },
        {
            "Answer": "B",
            "Explanation": "Entity analysis identifies key subjects in the text, including proper nouns and common nouns, allowing for automatic tagging.",
            "Image": "",
            "Option1": "Syntax analysis",
            "Option2": "Entity analysis",
            "Option3": "Sentiment analysis",
            "Option4": "Category analysis",
            "Question": "Your company wants to use AI to automatically tag the main words in legal documents. Which feature of the Natural Language API can help with this?",
            "Tags": [
                "Google Cloud ML Tools",
                "Entity Analysis",
                "Natural Language API"
            ]
        },
        {
            "Answer": "C",
            "Explanation": "The best approach is to define the optimal route as the shortest route that passes by all shuttle stations with confirmed attendance at the given time under capacity constraints, and then dispatch an appropriately sized shuttle indicating the required stops on the map.",
            "Image": "",
            "Option1": "Build a tree-based regression model that predicts how many passengers will be picked up at each shuttle station. 2. Dispatch an appropriately sized shuttle and provide the map with the required stops based on the prediction.",
            "Option2": "Build a tree-based classification model that predicts whether the shuttle should pick up passengers at each shuttle station. 2. Dispatch an available shuttle and provide the map with the required stops based on the prediction.",
            "Option3": "Define the optimal route as the shortest route that passes by all shuttle stations with confirmed attendance at the given time under capacity constraints. 2. Dispatch an appropriately sized shuttle and indicate the required stops on the map.",
            "Option4": "Build a reinforcement learning model with tree-based classification models that predict the presence of passengers at shuttle stops as agents and a reward function around a distance-based metric. 2. Dispatch an appropriately sized shuttle and provide the map with the required stops based on the simulated outcome.",
            "Question": "Your organization wants to make its internal shuttle service route more efficient. The shuttles currently stop at all pick-up points across the city every 30 minutes between 7 am and 10 am. The development team has already built an application on Google Kubernetes Engine that requires users to confirm their presence and shuttle station one day in advance. What approach should you take?",
            "Tags": [
                "Google Cloud ML Tools",
                "Model Training",
                "Model Deployment",
                "Optimization",
                "Google Kubernetes Engine"
            ]
        },
        {
            "Answer": "B",
            "Explanation": "Extending your test dataset with images of the newer products ensures that the evaluation metrics accurately reflect the performance of the model on both old and new products.",
            "Image": "",
            "Option1": "Keep the original test dataset unchanged even if newer products are incorporated into retraining.",
            "Option2": "Extend your test dataset with images of the newer products when they are introduced to retraining.",
            "Option3": "Replace your test dataset with images of the newer products when they are introduced to retraining.",
            "Option4": "Update your test dataset with images of the newer products when your evaluation metrics drop below a pre-decided threshold.",
            "Question": "You work for an online retail company that is creating a visual search engine. You have set up an end-to-end ML pipeline on Google Cloud to classify whether an image contains your company's product. Expecting the release of new products in the near future, you configured a retraining functionality in the pipeline so that new data can be fed into your ML models. You also want to use AI Platform's continuous evaluation service to ensure that the models have high accuracy on your test dataset. What should you do?",
            "Tags": [
                "Google Cloud ML Tools",
                "AI Platform",
                "Model Evaluation",
                "Continuous Evaluation"
            ]
        }
    ]
}